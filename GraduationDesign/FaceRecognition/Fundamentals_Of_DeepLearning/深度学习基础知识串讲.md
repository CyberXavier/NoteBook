# 深度学习基础知识串讲

## 2-1卷积神经网络基本概念

### 卷积神经网络内容概念

图1：

![2_1卷积神经网络内容概念](https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/FaceRecognition/Fundamentals_Of_DeepLearning/image/2_1%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%86%85%E5%AE%B9%E6%A6%82%E5%BF%B5.PNG?raw=true)

### 深度学习发展历程

图2：

<img src="https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/FaceRecognition/Fundamentals_Of_DeepLearning/image/2_1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B.PNG?raw=true" style="zoom:50%;" />

### 什么是人工神经网络？

* 属于机器学习的一种方法
  * 机器学习是解决人工智能问题，实现人工智能产品的非常重要的一项核心技术。其中最具代表性的就是深度学习

* 是一个网络的概念（包括网络节点）
  * 神经元    --->    网络节点（人工神经网络中的网络节点我们称之为神经元）

* 包括各种网络层
  * 例如： 输入层、输出层、隐藏层
  * 各个相邻层之间的神经元存在着连接关系
    * 连接关系：表示在上一层中有着连接关系的各个神经元通过一定的运算得到这一层神经元的值

### 什么是感知器？

* 一种学习算法
  * 第一个具有完整算法描述的神经网络学习算法（称为感知器学习算法：PLA）

* 任何线性分类和线性回归的任务都可以通过感知器来解决
  * 分类VS归回
    * 分类：通过神经网络预测出来的是一个离散的值
    * 回归：预测出来的是一个连续的值（例如：股票价格）

* 可以说是人工神经网络的一部分

### 从多层感知器到人工神经网络

* 多层感知器（MLP，Multilayer Perceptron）也叫人工神经网络(ANN，Artificial Neutral Network)

图3：

<img src="https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/FaceRecognition/Fundamentals_Of_DeepLearning/image/2-1%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E5%88%B0%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.PNG?raw=true" style="zoom: 67%;" />

### 什么是深度学习？

* 也是神经网络的一种

* 含多个隐藏层的多层感知器就是一种深度学习结构

### 总结

* 神经元	---->	感知器	---->	神经网络	---->	深度学习
  * 由多个神经元构成一个感知器，再由多个感知器构成一个神经网络，再由神经网络构中的隐藏层的层数加深就得到了深度学习
  * 多层感知器就是：神经网络
  * 多隐层的多层感知器就是：深度学习



## 2-2前向运算（即怎样使用当前的网络结构---怎么用）

### 运算条件

* 已知：网络结构+网络结构内部的参数

### 什么是前向运算

图1：

<img src="https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/FaceRecognition/Fundamentals_Of_DeepLearning/image/2-2%E5%89%8D%E5%90%91%E8%BF%90%E7%AE%97%E8%BE%93%E5%87%BA%E8%BF%87%E7%A8%8B.PNG?raw=true" style="zoom: 67%;" />

* 计算输出值的过程
  * 如图一右边的“f()函数”就是已经规定好的网络结构（即运算方式）
    * 例如：a1经过x1、x2、x3来进行线性组合，再通过“f()函数”进行映射之后得到了a1的值
    * 这里的w11、w12、w13、b也就是权值和偏置项的取值是不同的（它们的取值为多少也就是神经网络的参数是多少）



## 2-3反向传播（backpropagation,BP）（学到参数具体的值---怎么学）

* 神经网络（模型参数）训练方法	---	即BP算法可以求出参数值
  * 1986年由Rumelhar和hinton等人提出
  * 解决了神经网络优化的问题（可以求出各层参数的取值）
  * 计算输出层结果与真实值之间的偏差来进行逐层调节参数（用到了梯度下降的算法来调节）
    * 利用当前网络进行前向运算得到一个输出结果然后根据输出结果同真实数据的偏差来进行逐层调节参数

* 神经网络 = 网络结构 + 参数

  * 网络结构	---	网络中的各个层，例如CNN、RNN等
    * 网络结构的设计属于超参数【需要人为的去定义的参数】求解的问题
    * 目前工业界也有一些可以自动化设计网络结构的工具例如：Auto-ML、NAS

  * 参数（怎么学）    ---    在网络结构已知的条件下我们就需要使用BP算法去进行网络参数的求解
    * 通过输入有标签的样本【反向传播也是一个监督学习的过程】可以自动获取参数



## 2-4反向传播迭代过程以及参数优化概念

### 反向传播迭代过程

* 神经网络参数训练是一个不断迭代的过程（也称为训练的过程）

* ~~~c
  //迭代流程
  if(网络结构已知){
      参数初始化();
      前向运动();
      误差 = 计算LOSS();	//计算预测值与真实值之间的误差大小
      if(误差大){
          反向传播();	//梯度下降法调节参数减小到最小误差，
          再次回到前向运动();
      }else if(误差非常的小){
          得到最终神经网络需要用到的参数
      }
  }
  ~~~

* 流程图如下：

  <img src="https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/FaceRecognition/Fundamentals_Of_DeepLearning/image/2-4%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%AD%E4%BB%A3%E8%BF%87%E7%A8%8B.PNG?raw=true" alt="2-4反向传播迭代过程" style="zoom: 50%;" />

### 参数更新多少？

* 参数优化问题（也就是上面的迭代）
  * 可以通过导数和学习率来进行梯度下降法来优化参数



## 2-6反向传播之梯度下降法

* 沿着导数下降的方向，进行参数更新
* 选择适合的步长/学习率
  * 过大容易可能错过最优解或者不能收敛（例如：损失函数的值一直不断的震荡而找不到最小值）
  * 在最初始的时候学习率可以大一点（例如0.01）并且伴随神经网络的迭代过程来逐步调节学习率的值

* 局部最优解

  * 这是利用梯度下降法求解非凸函数经常会遇到的问题
    * 由于函数可能会出现许多极值点此时的导数都为0但是极值点不一定是最值点

  * 解决方法
    * 使用一个扰乱因子跳出局部
    * 从网络的初始化点着手，因为初始化点严重影响了找到最优解（例如在某个初始化位置所求得的局部最优解就是全局最优解）



## 2-7深度学习迅猛发展的原因

### 原因

* 2014年伴随着大数据的到来深度学习在非常多领域取得了重大的突破
* 如图所示：

<img src="https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/FaceRecognition/Fundamentals_Of_DeepLearning/image/2-7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%E8%BF%85%E7%8C%9B%E7%9A%84%E5%8E%9F%E5%9B%A0.PNG?raw=true" style="zoom: 50%;" />

### 常见深度学习模型

* 卷积神经网络（CNN）
* 循环神经网络（RNN）
* 自动编码机（Autoencoder）
* Restricted Boltzmann Machines（RBM，受限玻尔茨曼机）
* 深度信念网络（DBN，Deep Belief Network）
