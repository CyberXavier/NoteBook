### 1.Logistic Regression

* 逻辑回归也叫做对数几率回归（见西瓜书讲解）

### 2.Logistic Regression lost function(逻辑回归损失函数)

如图1-1：

![损失函数](https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/DeepLearning/AndrewNg/week2/Image/lostfunction.PNG)

【解析】：损失函数误差平方法：L(pre_y , rel_y) = 1/2(pre_y^2 - rel_y^2)^2	{梯度下降法不太好用所以再逻辑回归中用一个不同的损失函数，它看起来与误差平方相似的作用（见图1-1）并且损失函数只适用于单个训练样本}

---

### 3.Logistic Regression cost function

图1-2：

![成本函数](https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/DeepLearning/AndrewNg/week2/Image/%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0.png)

【解析】：成本函数基于参数的总成本。

----

### 4.Gradient Descent

图1-3

![逻辑回归梯度下降](https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/DeepLearning/AndrewNg/week2/Image/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.PNG)

【解析】：使用梯度下降找到使得代价最小的w、b的值。（因为我们要找到全局最优的解所以我们没有选择平方误差法因为这种方法会产生许多局部最优解。它不是一个凹函数）

---

### 5.Logistic regression derivatives

如下图1-4：

![链式求导法](https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/DeepLearning/AndrewNg/week2/Image/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95.PNG)

【解析】：这就是单个样本一次使用链式求导法则梯度下降的过程。[其中a代表pre_y；a=pre_y=德尔塔(z)=1/(1+e^-z)；L(a,y)=-(yloga+(1-y)log(1-a))]

---

### 6.SIMD

* juputer notebook 是使用的cpu而不是用gpu。

* > * CPU和GPU都有并行化指令有时会叫作SIMD（a single instruction multiple data）。
  >
  > * 【SIMD】：一条指令多个数据。
  >
  > > * SIMD：如果你使用了这样的内置函数np.function或者其他能让你去掉显示for loop的函数。
  > >
  > > * 这样python的numpy能够充分利用 并行化去更快计算。这一点对GPU和CPU上面计算都是成立的。只是GPU更加擅长SIMD计算但是CPU也不差。
  > > * 向量化能够加速你的代码。
  > > * 经验法则：只要有其他可能就不要使用显示for循环。

---

### 7.Vectorizing Logistic Regression（向量化逻辑回归）

如下图1-5：

![向量化](https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/DeepLearning/AndrewNg/week2/Image/%E5%90%91%E9%87%8F%E5%8C%96%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92week2.PNG)

【解析】：通过python的np.dot方法实现向量化去代替for loop。在求Z的式子中最后一项”+b“，在python中会自动转化成1×m的矩阵（通过python的广播（Broadcasting））。

---

### 8.向量化logistic回归的梯度输出

如下图1-6：

![](https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/DeepLearning/AndrewNg/week2/Image/%E5%90%91%E9%87%8F%E5%8C%96logistic%E5%BD%92%E5%9B%9E%E6%A2%AF%E5%BA%A6%E8%BE%93%E5%87%BA.PNG)

【解析】：同时计算m个训练数据的梯度，得到一个高效率的逻辑回归的实现（如下图1-7右边部分消除了for loop）。

图1-7：

![](https://github.com/small-k9/NoteBook/blob/main/GraduationDesign/DeepLearning/AndrewNg/week2/Image/%E9%AB%98%E6%95%88logistic%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0.PNG)

【解析】：如果你需要很多次梯度下降可能还需要在右边部分加上一个for loop。

---
