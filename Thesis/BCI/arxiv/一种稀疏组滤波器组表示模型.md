## Motor-imagery classification model for brain-computer interface: a sparse group filter bank representation model(脑机接口的运动图像分类模型:一种稀疏组滤波器组表示模型)



### 目录

* [背景](# 背景)
* [方法](# 提出的方法)
* [知识补充](# 知识补充)



### 背景

> 公共空间模式(Common spatial pattern, CSP)已被广泛应用于运动想象(MI)脑电图(EEG)记录的特征提取和脑机接口(BCI)的MI分类应用。BCI通常需要较长的脑电图数据来进行可靠的分类器训练。更具体地说，在使用一般空间模式进行特征提取之前，使用来自两个不同类的训练字典构造一个复合字典矩阵，并估计滤波器带内测试样本的表示为字典矩阵列的线性组合。

* 一种复合字典矩阵-->公共空间模式



### 提出的方法

> 为了缓解频带间稀疏小样本(SS)的问题。提出了一种新的运动图像稀疏群滤波组模型(SGFB)



### 结果

> 我们通过基于对应于非零相关系数的类别表示残差来执行任务。此外，我们还在三个不同的时间窗口中使用约束滤波器带执行联合稀疏优化，以在多任务学习框架中提取鲁棒的 CSP 特征。为了验证模型的有效性，我们在BCI竞赛的公共脑电图数据集上进行了实验，并与其他竞赛方法进行了比较。不同子带的良好分类性能证实我们的算法是改进基于 MI 的 BCI 性能的有希望的候选者。



### 知识补充

#### 目录

* [CSP](# 详解CSP)





# 详解CSP



1月 06, 2019 发布在 [脑-机接口算法大全](https://mrswolf.github.io/categories/脑-机接口算法大全/)

# 目录

1. 什么是CSP
   1. [CSP的历史](https://mrswolf.github.io/zh-cn/2019/01/06/详解CSP/#CSP的历史)
2. CSP（二分类）
   1. [CSP的原始形式](https://mrswolf.github.io/zh-cn/2019/01/06/详解CSP/#CSP的原始形式)
   2. [CSP的第二种表述](https://mrswolf.github.io/zh-cn/2019/01/06/详解CSP/#CSP的第二种表述)
   3. [CSP的第三种表述](https://mrswolf.github.io/zh-cn/2019/01/06/详解CSP/#CSP的第三种表述)
   4. [Talk is cheap. Show me the code.](https://mrswolf.github.io/zh-cn/2019/01/06/详解CSP/#Talk-is-cheap-Show-me-the-code)
3. [CSP（多分类）](https://mrswolf.github.io/zh-cn/2019/01/06/详解CSP/#CSP（多分类）)

## 什么是CSP

共空间模式(common spatial pattern,CSP)是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。目前，CSP及其改进算法的发展速度放缓，看似到达了算法的瓶颈期，近几年没有什么较大的突破。尽管如此，CSP中的一些数学思想对传统脑-机接口算法仍然具有较大的影响力，例如近年运用在SSVEP上的TRCA、DCPM等算法均和CSP有着异曲同工之妙。因此，本文从CSP原始算法出发，讨论其变形和一系列改进算法，试图为读者阐明其中的数学思想。

### CSP的历史

1970年，[Fukunaga和Koontz](https://www.computer.org/csdl/trans/tc/1970/04/01671511.pdf)在IEEE Transactions on Computers上发表论文，介绍了一种特征选择的方法，史称“Fukunaga-Koontz变换”，这种特征选择的方法迅速在各个领域得到推广。

1990年，[Koles等人](https://link.springer.com/article/10.1007/BF01129656)将“Fukunaga-Koontz变换”引入背景脑电分析，发现可以通过脑电有效的区分健康人群和精神病人群。

1999年，[MuÈller-Gerking和Pfurtscheller等人](https://www.sciencedirect.com/science/article/pii/S1388245798000388)（脑-机接口领域有名的Graz研究中心）把Koles的方法（他们对Koles的方法做了一些微小的改进）应用到运动分类上，并称这种方法为“common spatial pattern，CSP”。

次年，[Graz](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.323.7160&rep=rep1&type=pdf)小组的人又把CSP用在运动想象的分类上，取得比较好的分类效果，奠定了CSP在运动想象领域的地位。

## CSP（二分类）

2000年[Graz](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.323.7160&rep=rep1&type=pdf)的论文中提出的CSP是为二分类问题设计的，形式较为简单，然而如果你读CSP相关论文，就会发现CSP存在至少三种表述形式。这三种方式相互联系，又有所区分，很容易让初学者陷入混乱，不知道哪一种是正确形式。

为了解决这一问题，使读者更好的理解其中的数学本质，我接下来从2000年[Graz](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.323.7160&rep=rep1&type=pdf)论文中的算法出发，讨论三种形式间的联系和不同。当然，我假设读者具有线性代数知识（了解特征值分解，了解一些矩阵的操作方法和符号）以及一些基础的numpy或MATLAB数据处理经验，并请读者牢记以下格言

> DON’T PANIC
>
> **The Hitchhiker's Guide to the Galaxy**Douglas Adams

### CSP的原始形式

假设我们做脑电实验，采集到两类不同的任务信号，用矩阵形式可以表示为E(i)1∈RNc×SE1(i)∈RNc×S和E(i)2∈RNc×SE2(i)∈RNc×S，其中NcNc表示脑电导联数目，SS表示采样点的个数，而上标(i)(i)表示试次的序号（脑电实验通常会对一种任务进行多次实验，得到很多试次的数据），原始CSP算法采用以下步骤：

**第一步**，对数据做decenter处理，即减去每一导联在样本点上的均值

E(i)1=E(i)1−mean(E(i)1,axis=1)E(i)2=E(i)2−mean(E(i)2,axis=1)E1(i)=E1(i)−mean(E1(i),axis=1)E2(i)=E2(i)−mean(E2(i),axis=1)

**第二步**，求每一试次的协方差矩阵并归一化，最后得到平均的协方差矩阵

¯C1=∑iE(i)1(E(i)1)Ttrace(E(i)1(E(i)1)T)¯C2=∑iE(i)2(E(i)2)Ttrace(E(i)2(E(i)2)T)C¯1=∑iE1(i)(E1(i))Ttrace(E1(i)(E1(i))T)C¯2=∑iE2(i)(E2(i))Ttrace(E2(i)(E2(i))T)

这里的函数trace()trace()是求矩阵的迹，即主对角线上的元素之和。注意到对于去均值的矩阵EE，其协方差矩阵可以表示为C=1S−1EETC=1S−1EET，上式中没有S−1S−1出现是因为上下相除互相抵消。¯C1C¯1和¯C2C¯2则是两类任务信号的平均协方差矩阵。

> 为什么要使用trace()trace()来对协方差矩阵归一化？
>
> 1990年Koles的文章中指出，归一化的目的是为了消除”被试间脑电信号幅值的变化”，注意到Koles的原意是区分健康人群和精神疾病人群，而个体的脑电幅值是有绝对性的差异的。方差可以表征信号在时域上的能量高低，不同人群的协方差矩阵的绝对值不同。为了消除这种差异带来的影响，利用trace()trace()函数求得所有导联的总体能量，并对协方差矩阵归一化，从而安排除不同个体带来的干扰。Graz小组对同一个体不同试次的数据沿用了这种归一化方式，试图消除试次间的差异，发现也有一定的作用，这种归一化方式就一直流传下来。
>
> 然而，有些分析显示这种归一化方式反而会削弱信号的可分性，建议不要使用归一化。关于这一点，我会在以后进行讨论。

**第三步**， 构建复合协方差矩阵，并特征值分解，构建白化（whitening）矩阵

Cc=¯C1+¯C2Cc=VcDcVTc=[v(c)1v(c)2⋯v(c)Nc]⎡⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢⎣λ(c)1λ(c)2⋱λ(c)Nc⎤⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥⎦⎡⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢⎣v(c)1v(c)2⋮v(c)Nc⎤⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥⎦P=D−1/2cVTcCc=C¯1+C¯2Cc=VcDcVcT=[v1(c)v2(c)⋯vNc(c)][λ1(c)λ2(c)⋱λNc(c)][v1(c)v2(c)⋮vNc(c)]P=Dc−1/2VcT

其中VcVc是特征向量矩阵（每一列是特征向量），DcDc是由特征值组成的对角矩阵。PP是白化矩阵，其目的是把对角矩阵DcDc变化为单位矩阵II,即PCcPT=IPCcPT=I成立。

**第四步**，计算空间滤波器WW

I=PCcPT=P¯C1PT+P¯C2PT=S1+S2S1=VD1VT=[v1v2⋯vNc]⎡⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢⎣λ(1)1λ(1)2⋱λ(1)Nc⎤⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥⎦⎡⎢ ⎢ ⎢ ⎢⎣v1v2⋮vNc⎤⎥ ⎥ ⎥ ⎥⎦S2=VD2VT=[v1v2⋯vNc]⎡⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢⎣λ(2)1λ(2)2⋱λ(2)Nc⎤⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥⎦⎡⎢ ⎢ ⎢ ⎢⎣v1v2⋮vNc⎤⎥ ⎥ ⎥ ⎥⎦W=PTVI=PCcPT=PC¯1PT+PC¯2PT=S1+S2S1=VD1VT=[v1v2⋯vNc][λ1(1)λ2(1)⋱λNc(1)][v1v2⋮vNc]S2=VD2VT=[v1v2⋯vNc][λ1(2)λ2(2)⋱λNc(2)][v1v2⋮vNc]W=PTV

其中矩阵S1S1和S2S2具有同样的特征向量VV（这也是共空间模式名称的由来），而相对应的特征值相加始终为1，即D1+D2=ID1+D2=I。

> 为什么S1S1和S2S2具有同样的特征向量和此消彼长的特征值关系？
> 这一点可以简单的证明如下
> 假设vjvj和λ(1)jλj(1)分别是S1S1的特征向量和特征值，即
>
> S1vj=λ(1)jvjS1vj=λj(1)vj
>
> 注意到S1+S2=IS1+S2=I，把上式中的S1S1置换掉可得
>
> (I−S2)vj=λ(1)jvj(I−S2)vj=λj(1)vj
>
> 把上式变形一下可得
>
> S2vj=(1−λ(1)j)vjS2vj=(1−λj(1))vj
>
> 显然vjvj也是S2S2的特征向量，只不过其特征值为1−λ(1)j1−λj(1)

这里还有一点需要注意，我们假设S1S1特征值的顺序是按降序排列的（那么S2S2的特征值就是按升序排列），即

1≥λ(1)1≥λ(1)2≥⋯≥λ(1)Nc≥00≤λ(2)1≤λ(2)2≤⋯≤λ(2)Nc≤11≥λ1(1)≥λ2(1)≥⋯≥λNc(1)≥00≤λ1(2)≤λ2(2)≤⋯≤λNc(2)≤1

这种排序的主要目的是为了以后分析的便利性，例如在运动想象分类中提取最有效的空间滤波器。

> 协方差矩阵是半正定矩阵（positive semidefinite），而半正定矩阵的特征值均为非负。故S1S1和S2S2的特征值在0~1之间



以上就是原始CSP算法的基本内容，在得到空间滤波器矩阵WW后（WW的每一列都是一个空间滤波器），就可以对信号进行变换Z=WTEZ=WTE，而ZZ的每一行则代表了滤波后的一个时序特征信号，接下来便可以对ZZ做进一步的分析。

简单回顾一下CSP算法，不难发现CSP实质求解的是这样一个问题，寻找正交矩阵WW使得以下条件成立：

WT¯C1W=D1WT¯C2W=D2D1+D2=IWTC¯1W=D1WTC¯2W=D2D1+D2=I

让我们对以上的公式做一些变换，把第一个和第二个公式相加

WT(¯C1+¯C2)W=D1+D2=IWT(C¯1+C¯2)W=D1+D2=I

又因为WW是正交矩阵，故WT=W−1WT=W−1，从而

(¯C1+¯C2)W=W(C¯1+C¯2)W=W

把上式代入¯C1W=WD1C¯1W=WD1，可得

¯C1W=(¯C1+¯C2)WD1C¯1W=(C¯1+C¯2)WD1

这个式子是不是看起来很像特征向量定义的公式¯C1W=WD1C¯1W=WD1呢？只不过等式右边多了一个矩阵¯C1+¯C2C¯1+C¯2。

这类形式的问题叫[广义特征值问题](http://fourier.eng.hmc.edu/e161/lectures/algebra/node7.html)，求解广义特征值问题是脑-机接口领域传统空间滤波方法的基础，大量的算法（CSP、TRCA等）都可以转化为这一形式。

下一小节中，我将从¯C1W=(¯C1+¯C2)WD1C¯1W=(C¯1+C¯2)WD1这一公式出发来探讨CSP的第二种表述形式。

### CSP的第二种表述

在讨论CSP的第二种表述之前，我们需要了解一个数学概念[广义雷利商（generalized Rayleigh quotient）](https://en.wikipedia.org/wiki/Rayleigh_quotient)。

广义雷利商长这样

R=wTAwwTBwA,B⪰0R=wTAwwTBwA,B⪰0

其中AA和BB为半正定矩阵（读者可以简单理解为协方差矩阵），ww是列向量，显然广义雷利商RR是一个实数。

如果我们求如下广义雷利商的优化问题，就会有一些有趣的结果

maxwwTAwwTBwmaxwwTAwwTBw

寻找ww使得RR最大，在数学上可以等价为求解下式（我就不证明了，感兴趣的读者可点击广义雷利商的链接查看证明过程）

Aw=Bwλ1Aw=Bwλ1

这个公式就是上一节提到的广义特征值问题，也就是说，寻找ww使广义雷利商最大可以等价为求解AA和BB的广义特征值问题并找到使特征值λ1λ1最大所对应的特征向量ww。

如果我们继续寻找能够使RR第二大、第三大的ww，就会发现只要解出广义特征值问题的矩阵形式即可

AW=BWD=B[w1w2⋯wN]⎡⎢ ⎢ ⎢ ⎢ ⎢⎣λ1λ2⋱λN⎤⎥ ⎥ ⎥ ⎥ ⎥⎦AW=BWD=B[w1w2⋯wN][λ1λ2⋱λN]

其中λ1≥λ2≥⋯≥λNλ1≥λ2≥⋯≥λN, 特征值的数值也就是广义雷利商的数值。

如果读者明白广义雷利商和广义特征值问题之间的关联，就不难发现，上一节中推导的CSP求解问题可以变形为求解广义雷利商问题

¯C1W=(¯C1+¯C2)WD1 ⟺ maxwwT¯C1wwT(¯C1+¯C2)wC¯1W=(C¯1+C¯2)WD1 ⟺ maxwwTC¯1wwT(C¯1+C¯2)w

这里A=¯C1A=C¯1、B=¯C1+¯C2B=C¯1+C¯2，WW矩阵是广义雷利商第一大、第二大至第N大向量ww组成的集合。

这里我们推出了CSP问题的第二种表述形式，即

maxwwT¯C1wwT(¯C1+¯C2)wmaxwwTC¯1wwT(C¯1+C¯2)w

### CSP的第三种表述

CSP的第三种表述形式需要绕点弯路。首先还是从CSP的原始形式出发，即寻找正交矩阵WW使得以下条件成立：

WT¯C1W=D1WT¯C2W=D2D1+D2=IWTC¯1W=D1WTC¯2W=D2D1+D2=I

在第二个公式的左右两边同时右乘矩阵W−1¯C−12W−1C¯2−1，可以得到

WT=D2W−1¯C−12WT=D2W−1C¯2−1

把该式代入WT¯C1W=D1WTC¯1W=D1，替换掉WTWT，可得

D2W−1¯C−12¯C1W=D1D2W−1C¯2−1C¯1W=D1

上式左右两边左乘¯C2WD−12C¯2WD2−1，可得

¯C1W=¯C2WD−12D1=¯C2WΛΛ=D−12D1C¯1W=C¯2WD2−1D1=C¯2WΛΛ=D2−1D1

没错，我们又推出了熟悉的广义特征值问题¯C1W=¯C2WΛC¯1W=C¯2WΛ，再考虑广义雷利商与之的联系，可以得到CSP的第三种表述

maxwwT¯C1wwT¯C2wmaxwwTC¯1wwTC¯2w

> 相比CSP的原始形式和第二种表述形式，第三种表述形式更适合从直观上解释CSP在运动想象上有效的原因。
> 运动想象会产生事件相关同步（ERS）和事件相关去同步（ERD）的现象，简单来说就是从电信号上看，某些脑区能量升高，某些脑区能量降低，故能量变化才是运动想象分类的关键特征。
>
> 我们前面提高过，方差可以看作一导信号能量的高低（协方差矩阵则是多导信号的能量反应），因此CSP的第三种表述形式实质体现这样一个问题：
> 寻找一种变换方式ww，使得变换后任务1的能量（wT¯C1wwTC¯1w）和任务2的能量（wT¯C2wwTC¯2w）差异最大化（其比值最大）。
>
> CSP的这种特性恰好和运动想象的现象一致，CSP对能量特征做转换，强化了不同任务间能量差异。

关于CSP的第三种表述，最后还需要注意的一点是其同CSP原始形式和第二种表述形式并不完全等价，我们在推导第三种表述形式过程种始终没有用到这样一个约束条件D1+D2=ID1+D2=I。

这表明，第三种形式是CSP的一种泛化形式，其和CSP原始形式和第二种表述的差异仅在于特征值ΛΛ不要求在0~1的范围内，具体来说，它们的特征值间存在这样一种关系

Λ=D−12D1D1=Λ(Λ+I)−1D2=(Λ+I)−1Λ=D2−1D1D1=Λ(Λ+I)−1D2=(Λ+I)−1

### Talk is cheap. Show me the code.

TL;DR
我的实践经验表明，CSP第二种和第三种形式总要优于原始形式，主要是能够避免很多数值精度产生的问题。

如果你想试试原生的CSP，选第二种形式。

如果你能够理解其本质，第三种形式或许更合适(我喜欢第三种形式，因为从中可以引出CSP和Riemannian Geometry的关系，这一点以后再谈)

以下是三种CSP的代码

```
import numpy as np
import scipy.linalg as linalg

# generate data
X1 = np.random.rand(20, 60, 1000)
X2 = np.random.rand(20, 60, 1000)

def csp1(X1, X2):
    '''
    The first form of CSP.
    X1: # of trials, # of channels, # of samples
    X2: # of trials, # of channels, # of samples
    
    Return
    W: spatial fitlers
    D1: eigenvalues of filters
    ''' 
    X1 = X1 - X1.mean(axis=2, keepdims=True)
    X2 = X2 - X2.mean(axis=2, keepdims=True)
    # normalization covariance
    C1 = []
    C2 = []
    for i in range(X1.shape[0]):
        tmp = X1[i,:,:].dot(X1[i,:,:].T)
        tmp = tmp/np.trace(tmp)
        C1.append(tmp)
    for i in range(X2.shape[0]):
        tmp = X2[i,:,:].dot(X2[i,:,:].T)
        tmp = tmp/np.trace(tmp)
        C2.append(tmp)
    C1 = np.array(C1)
    C2 = np.array(C2)
    # average covariance
    mean_C1 = C1.mean(axis=0)
    mean_C2 = C2.mean(axis=0)
    mean_C = mean_C1 + mean_C2
    # whitening matrix
    D_c, V_c = linalg.eigh(mean_C)
    isqrt_D_c = np.diag(np.sqrt(1/D_c))
    P = isqrt_D_c@V_c.T
    # S1 and S2
    S1 = P@mean_C1@P.T
    S2 = P@mean_C2@P.T
    # spatial filters
    D1, V = linalg.eigh(S1)
    W = P.T@V
    return W, D1

def csp2(X1, X2):
    '''
    The second form of CSP.
    X1: # of trials, # of channels, # of samples
    X2: # of trials, # of channels, # of samples
    
    Return
    W: spatial fitlers
    D1: eigenvalues of filters
    ''' 
    X1 = X1 - X1.mean(axis=2, keepdims=True)
    X2 = X2 - X2.mean(axis=2, keepdims=True)
    # normalization covariance
    C1 = []
    C2 = []
    for i in range(X1.shape[0]):
        tmp = X1[i,:,:].dot(X1[i,:,:].T)
        tmp = tmp/np.trace(tmp)
        C1.append(tmp)
    for i in range(X2.shape[0]):
        tmp = X2[i,:,:].dot(X2[i,:,:].T)
        tmp = tmp/np.trace(tmp)
        C2.append(tmp)
    C1 = np.array(C1)
    C2 = np.array(C2)
    # average covariance
    mean_C1 = C1.mean(axis=0)
    mean_C2 = C2.mean(axis=0)
    mean_C = mean_C1 + mean_C2
    # generalized eigenvalue problem
    D1, W = linalg.eigh(mean_C1,mean_C)
    return W, D1

def csp3(X1, X2):
    '''
    The third form of CSP
    X1: # of trials, # of channels, # of samples
    X2: # of trials, # of channels, # of samples
    
    Return
    W: spatial fitlers
    D: eigenvalues of filters
    ''' 
    X1 = X1 - X1.mean(axis=2, keepdims=True)
    X2 = X2 - X2.mean(axis=2, keepdims=True)
    # normalization covariance
    C1 = []
    C2 = []
    for i in range(X1.shape[0]):
        tmp = X1[i,:,:].dot(X1[i,:,:].T)
        tmp = tmp/np.trace(tmp)
        C1.append(tmp)
    for i in range(X2.shape[0]):
        tmp = X2[i,:,:].dot(X2[i,:,:].T)
        tmp = tmp/np.trace(tmp)
        C2.append(tmp)
    C1 = np.array(C1)
    C2 = np.array(C2)
    # average covariance
    mean_C1 = C1.mean(axis=0)
    mean_C2 = C2.mean(axis=0)
    D, W = linalg.eigh(mean_C1, mean_C2)
    return W, D
```



```
W1, D1 = csp1(X1, X2)
W2, D2 = csp2(X1, X2)
W3, D3 = csp3(X1, X2)
```

让我们看一下CSP原始形式的第一个空间滤波器的特征值

```
D1[0]
0.4601822497625514
```

再看一下CSP第二种表述形式的第一个空间滤波器的特征值，两者大致相同，但最后几位不太一样，这是由于数值舍入精度的问题（或许跟如何实施算法有关），关于这方面的问题我也在研究中，目前就忽略吧。

```
D2[0]
0.46018224976254957
```

再看一下CSP第三种表述形式的第一个空间滤波器的特征值，好像跟上面两者差别很大呢。

```
D3[0]
0.8524770620457159
```

回顾一下前文讨论的特征值间的联系，简单的做个变换，发现我们的算法是正确的。

```
# connection between form2 and form3
new_D = D3/(D3+1)
new_D[0]
0.4601822497625497
```

最后来看下空间滤波器（特征向量）是否正确，我们来检验第一特征向量的前5个数值。

```
W1[:5, 0]
array([ 1.56843115,  0.71217374, -0.50926103,  0.52207783, -0.29789071])
```

第二种形式和CSP原始形式完全一致（有时会差一个正负号，但从算法上来说是完全正确的）

```
# maybe negative, but that's ok
W2[:5, 0]
array([ 1.56843115,  0.71217374, -0.50926103,  0.52207783, -0.29789071])
```

看一下第三种形式。咦，好像不太对哦。

```
# doesn't look like the result above. Is there something wrong?
W3[:5, 0]
array([ 2.13472472,  0.9693093 , -0.69313345,  0.71057786, -0.40544634])
```

别慌，第三种形式只和第一种和第二种差了一个倍数（向量是一样的），做如下变换就能得到一样的结果。

```
# Nothing is wrong here.
np.sqrt(1/(D3+1))[0]*W3[:5, 0]
array([ 1.56843115,  0.71217374, -0.50926103,  0.52207783, -0.29789071])
```

## CSP（多分类）

**未完待续**